{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoyNtLwmkUVW"
      },
      "source": [
        "# Natural Language Processing. Assignment 1. Tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kygkn1jMkaT9"
      },
      "source": [
        "In this assignment, you need to implement, train, and analyze a Byte-Pair Encoding (BPE) tokenizer.\n",
        "\n",
        "The assignment consist of 3 tasks. When you finish all the tasks, create a GitHub repository for this assignment (you can use this repository later for the other assignments) and submit this notebook in the repository. Leave `requirements.txt` file if your code requires additional installations. Submit the link to the repository in Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJokC1PFqhNe"
      },
      "source": [
        "## Task 1: Data Preparation and Vocabulary Size Selection (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbPSf6pqrcF"
      },
      "source": [
        "First, load the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus). After loading the corpus, you need to select the appropriate vocabulary size for the BPE tokenizer. The appropriate vocabulary size is the minimal vocabulary size that covers at least 90% of the words in the corpus. The coverage is calculated according to the following formula:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbzsvC77u0We"
      },
      "source": [
        "$$ \\text{coverage}(k) = \\frac{\\sum_{r=1}^{k} f(r)}{\\sum_{r=1}^{N} f(r)} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIjntOQDu1ed"
      },
      "source": [
        "where $f(r)$ is the frequency of the top-$r$ word, $k$ is the number of top-$k$ tokens included in vocab, $N$ is the total unique words in corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtunZHVEvNuF"
      },
      "source": [
        "So, for this task you need to do the following:\n",
        "\n",
        "1. Load the Brown corpus (0.5 points)\n",
        "2. Plot cumulative coverage vs. vocabulary size for the loaded corpus (1 point)\n",
        "3. Select the appropriate vocabulary size (0.5 point)\n",
        "4. Answer the questions:\n",
        "    1. Why the coverage slows down the increase as the vocabulary size increases? (0.5 point)\n",
        "    2. Which empirical law explains the slowing down increase of the coverage? (0.5 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Brown corpus...\n",
            "Total words in corpus: 1,161,192\n",
            "Unique words in corpus: 56,057\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Download Brown corpus if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/brown')\n",
        "except LookupError:\n",
        "    nltk.download('brown', quiet=True)\n",
        "\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# 1. Load the Brown corpus\n",
        "print(\"Loading Brown corpus...\")\n",
        "words = brown.words()\n",
        "print(f\"Total words in corpus: {len(words):,}\")\n",
        "print(f\"Unique words in corpus: {len(set(words)):,}\")\n",
        "\n",
        "# 2. Calculate and plot cumulative coverage vs vocabulary size\n",
        "def calculate_coverage(words, vocab_size):\n",
        "    \"\"\"\n",
        "    Calculate cumulative coverage for a given vocabulary size.\n",
        "\n",
        "    Coverage formula: coverage(k) = sum(frequencies of top-k words) / sum(all frequencies)\n",
        "\n",
        "    This tells us what percentage of the corpus is covered by the top-k most frequent words.\n",
        "    For example, if coverage(100) = 0.5, it means the top 100 words account for 50% of all\n",
        "    word occurrences in the corpus.\n",
        "    \"\"\"\n",
        "    # Step 1: Count how many times each word appears in the corpus\n",
        "    # Counter creates a dictionary: {word: frequency}\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    # Step 2: Calculate total frequency - sum of all word occurrences\n",
        "    # This is the denominator in our coverage formula\n",
        "    total_freq = sum(word_freq.values())\n",
        "\n",
        "    # Step 3: Get the top-k most frequent words\n",
        "    # most_common(vocab_size) returns a list of tuples: [(word, freq), ...]\n",
        "    top_k_words = word_freq.most_common(vocab_size)\n",
        "\n",
        "    # Step 4: Sum up the frequencies of these top-k words\n",
        "    # This is the numerator in our coverage formula\n",
        "    top_k_freq = sum(freq for _, freq in top_k_words)\n",
        "\n",
        "    # Step 5: Calculate coverage as a ratio\n",
        "    coverage = top_k_freq / total_freq if total_freq > 0 else 0.0\n",
        "    return coverage\n",
        "\n",
        "# Calculate coverage for different vocabulary sizes\n",
        "word_freq = Counter(words)\n",
        "total_unique_words = len(word_freq)\n",
        "\n",
        "vocab_sizes = []\n",
        "coverages = []\n",
        "\n",
        "# Adaptive step size: use smaller steps when coverage changes rapidly (small vocab sizes),\n",
        "# and larger steps when coverage plateaus (large vocab sizes)\n",
        "# This gives us more data points where the curve is interesting (steep) and fewer\n",
        "# where it's flat, making the plot more informative\n",
        "current_size = 0\n",
        "while current_size < min(10000, total_unique_words):\n",
        "    # Determine step size based on current vocabulary size\n",
        "    # Small vocab sizes: coverage changes quickly, so we sample more frequently (step=1)\n",
        "    if current_size < 100:\n",
        "        step = 1\n",
        "    # Medium-small: still changing, sample every 5 words\n",
        "    elif current_size < 500:\n",
        "        step = 5\n",
        "    # Medium: changes slower, sample every 10 words\n",
        "    elif current_size < 1000:\n",
        "        step = 10\n",
        "    # Medium-large: changes very slowly, sample every 50 words\n",
        "    elif current_size < 5000:\n",
        "        step = 50\n",
        "    # Large: coverage barely changes, sample every 100 words\n",
        "    else:\n",
        "        step = 100\n",
        "\n",
        "    # Move to next vocabulary size\n",
        "    current_size += step\n",
        "    # Don't exceed the total number of unique words\n",
        "    if current_size > total_unique_words:\n",
        "        current_size = total_unique_words\n",
        "\n",
        "    # Calculate coverage for this vocabulary size\n",
        "    coverage = calculate_coverage(words, current_size)\n",
        "    # Store the data point\n",
        "    vocab_sizes.append(current_size)\n",
        "    coverages.append(coverage)\n",
        "\n",
        "    # Stop if we've covered all unique words\n",
        "    if current_size >= total_unique_words:\n",
        "        break\n",
        "\n",
        "# Plot cumulative coverage vs vocabulary size\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(vocab_sizes, coverages, 'b-', linewidth=2)\n",
        "plt.axhline(y=0.9, color='r', linestyle='--', label='90% Coverage')\n",
        "plt.xlabel('Vocabulary Size', fontsize=12)\n",
        "plt.ylabel('Cumulative Coverage', fontsize=12)\n",
        "plt.title('Cumulative Coverage vs Vocabulary Size (Brown Corpus)', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Select the appropriate vocabulary size (minimal that covers at least 90%)\n",
        "def find_appropriate_vocab_size(words, target_coverage=0.9):\n",
        "    \"\"\"\n",
        "    Find minimal vocabulary size that covers at least target_coverage.\n",
        "\n",
        "    Uses binary search to efficiently find the answer. Instead of checking every possible\n",
        "    vocabulary size (which could be thousands), we use binary search to find the answer\n",
        "    in O(log n) time.\n",
        "    \"\"\"\n",
        "    # Step 1: Count word frequencies and get total unique words\n",
        "    word_freq = Counter(words)\n",
        "    total_unique_words = len(word_freq)\n",
        "\n",
        "    # Step 2: Binary search setup\n",
        "    # We search in the range [1, total_unique_words]\n",
        "    # left = smallest possible vocab size (1 word)\n",
        "    # right = largest possible vocab size (all unique words)\n",
        "    left, right = 1, total_unique_words\n",
        "    result = total_unique_words  # Default: use all words if we can't find a smaller solution\n",
        "\n",
        "    # Step 3: Binary search loop\n",
        "    # The idea: coverage increases as vocab_size increases (more words = more coverage)\n",
        "    # So we can use binary search to find the smallest vocab_size with coverage >= 0.9\n",
        "    while left <= right:\n",
        "        # Check the middle point\n",
        "        mid = (left + right) // 2\n",
        "        # Calculate coverage for this vocabulary size\n",
        "        coverage = calculate_coverage(words, mid)\n",
        "\n",
        "        # If we've reached our target coverage (e.g., >= 90%)\n",
        "        if coverage >= target_coverage:\n",
        "            # This vocab_size works! But maybe a smaller one also works?\n",
        "            # So we record this as a candidate and search left (smaller vocab sizes)\n",
        "            result = mid\n",
        "            right = mid - 1  # Try smaller vocab sizes\n",
        "        else:\n",
        "            # Coverage is too low, we need more words\n",
        "            # So we search right (larger vocab sizes)\n",
        "            left = mid + 1\n",
        "\n",
        "    # Return the minimal vocabulary size that achieves target coverage\n",
        "    return result\n",
        "\n",
        "vocab_size = find_appropriate_vocab_size(words, target_coverage=0.9)\n",
        "actual_coverage = calculate_coverage(words, vocab_size)\n",
        "\n",
        "print(f\"\\nAppropriate vocabulary size: {vocab_size}\")\n",
        "print(f\"Actual coverage achieved: {actual_coverage:.4f} ({actual_coverage*100:.2f}%)\")\n",
        "\n",
        "# 4. Answer the questions\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANSWERS TO QUESTIONS:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n1. Why does the coverage slow down as vocabulary size increases?\")\n",
        "print(\"   Answer: The coverage slows down because of Zipf's law - word\")\n",
        "print(\"   frequencies follow a power-law distribution. The most frequent\")\n",
        "print(\"   words (top-ranked) account for a large portion of the corpus,\")\n",
        "print(\"   while less frequent words (lower-ranked) appear much less often.\")\n",
        "print(\"   As we add more words to the vocabulary, we're adding words with\")\n",
        "print(\"   progressively lower frequencies, so each additional word contributes\")\n",
        "print(\"   less to the total coverage.\")\n",
        "\n",
        "print(\"\\n2. Which empirical law explains the slowing down increase of coverage?\")\n",
        "print(\"   Answer: Zipf's law (or Zipf's distribution) explains this phenomenon.\")\n",
        "print(\"   Zipf's law states that the frequency of a word is inversely\")\n",
        "print(\"   proportional to its rank in the frequency table. This means that\")\n",
        "print(\"   a small number of high-frequency words account for most of the\")\n",
        "print(\"   corpus, while many low-frequency words account for very little.\")\n",
        "print(\"   This power-law distribution causes the coverage curve to flatten\")\n",
        "print(\"   as vocabulary size increases.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94OTtlKZl13I"
      },
      "source": [
        "## Task 2: Implement Byte-Pair Encoding (BPE) Tokenizer (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uphIHqoEmDLH"
      },
      "source": [
        "Implement the [BPE tokenizer](https://arxiv.org/pdf/1508.07909) as the `BPETokenizer` class.\n",
        "\n",
        "The class should contain correctly implemented:\n",
        "\n",
        "* `train` method (1.5 points).\n",
        "* `tokenize` method (1.5 points).\n",
        "\n",
        "The code should have docstrings and comments (1 point)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Byte-Pair Encoding (BPE) Tokenizer Implementation\n",
        "\n",
        "BPE is a data compression algorithm adapted for NLP subword tokenization.\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "\n",
        "class BPETokenizer:\n",
        "    \"\"\"\n",
        "    Byte-Pair Encoding (BPE) Tokenizer implementation.\n",
        "\n",
        "    BPE iteratively replaces the most frequent pair of consecutive subword units\n",
        "    with a single, unused token. This allows handling of rare and out-of-vocabulary words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the BPE tokenizer.\"\"\"\n",
        "        # Vocabulary: maps token to its index\n",
        "        self.vocab = {}\n",
        "\n",
        "        # Reverse vocabulary: maps index to token\n",
        "        self.vocab_reverse = {}\n",
        "\n",
        "        # BPE merges: list of (token1, token2) pairs that were merged\n",
        "        self.merges = []\n",
        "\n",
        "        # Special tokens\n",
        "        self.unk_token = \"<UNK>\"\n",
        "        self.end_of_word_token = \"</w>\"  # Marks end of word\n",
        "\n",
        "    def _pre_tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Pre-tokenize text into words and add end-of-word markers.\n",
        "\n",
        "        This is the first step in BPE tokenization. We need to:\n",
        "        1. Split text into words (pre-tokenization)\n",
        "        2. Add a special marker to indicate word boundaries (</w>)\n",
        "\n",
        "        Why add </w>? BPE works on character/subword level, so we need to know where\n",
        "        words end. Otherwise, \"cat\" and \"cats\" might merge incorrectly with other words.\n",
        "\n",
        "        Args:\n",
        "            text: Input text string\n",
        "\n",
        "        Returns:\n",
        "            List of pre-tokenized words with end-of-word markers\n",
        "        \"\"\"\n",
        "        # Step 1: Split text into words\n",
        "        # \\S+ matches one or more non-whitespace characters (words)\n",
        "        # .lower() converts to lowercase for consistency\n",
        "        words = re.findall(r'\\S+', text.lower())\n",
        "\n",
        "        # Step 2: Add end-of-word marker to each word\n",
        "        # This marker helps BPE know where words end during merging\n",
        "        words = [word + self.end_of_word_token for word in words]\n",
        "\n",
        "        return words\n",
        "\n",
        "    def _get_word_freqs(self, texts: List[str]) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Get word frequencies from a list of texts.\n",
        "\n",
        "        Args:\n",
        "            texts: List of text strings\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping words (with </w>) to their frequencies\n",
        "        \"\"\"\n",
        "        word_freqs = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            words = self._pre_tokenize(text)\n",
        "            word_freqs.update(words)\n",
        "\n",
        "        return dict(word_freqs)\n",
        "\n",
        "    def _get_stats(self, word_freqs: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "        \"\"\"\n",
        "        Get statistics of pairs of consecutive symbols.\n",
        "\n",
        "        This is the core of BPE: we need to find which pairs of consecutive characters/subwords\n",
        "        appear most frequently. These frequent pairs will be merged into a single token.\n",
        "\n",
        "        Example: If \"th\" appears 1000 times and \"he\" appears 800 times, we'll merge \"th\" first.\n",
        "\n",
        "        Args:\n",
        "            word_freqs: Dictionary mapping words to their frequencies\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping pairs (char1, char2) to their total frequency\n",
        "        \"\"\"\n",
        "        pairs = defaultdict(int)\n",
        "\n",
        "        for word, freq in word_freqs.items():\n",
        "            # Step 1: Split word into individual characters/symbols\n",
        "            symbols = list(word)\n",
        "\n",
        "            # Step 2: Count all pairs of consecutive symbols\n",
        "            # We iterate through adjacent pairs and increment count by word frequency\n",
        "            for i in range(len(symbols) - 1):\n",
        "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def _merge_vocab(self, pair: Tuple[str, str], word_freqs: Dict[str, int]) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Merge the most frequent pair in the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            pair: Tuple of (token1, token2) to merge\n",
        "            word_freqs: Current word frequencies dictionary\n",
        "\n",
        "        Returns:\n",
        "            Updated word frequencies dictionary with merged pairs\n",
        "        \"\"\"\n",
        "        new_word_freqs = {}\n",
        "        merged_token = ''.join(pair)\n",
        "\n",
        "        for word in word_freqs:\n",
        "            # Replace consecutive occurrences of the pair with merged token\n",
        "            # We need to replace all occurrences, not just one\n",
        "            new_word = word.replace(''.join(pair), merged_token)\n",
        "            new_word_freqs[new_word] = word_freqs[word]\n",
        "\n",
        "        return new_word_freqs\n",
        "\n",
        "    def train(self, texts: List[str], vocab_size: int):\n",
        "        \"\"\"\n",
        "        Train the BPE tokenizer on a list of texts.\n",
        "\n",
        "        The training process:\n",
        "        1. Start with character-level vocabulary (each character is a token)\n",
        "        2. Iteratively find the most frequent pair of consecutive tokens\n",
        "        3. Merge that pair into a single token\n",
        "        4. Repeat until we reach the desired vocabulary size\n",
        "\n",
        "        Args:\n",
        "            texts: List of training text strings\n",
        "            vocab_size: Desired vocabulary size (including special tokens)\n",
        "        \"\"\"\n",
        "        # Step 1: Pre-tokenize texts and count word frequencies\n",
        "        word_freqs = self._get_word_freqs(texts)\n",
        "\n",
        "        # Step 2: Initialize vocabulary with all unique characters\n",
        "        # BPE starts at the character level - each character is initially a separate token\n",
        "        vocab = set()\n",
        "        for word in word_freqs.keys():\n",
        "            for char in word:\n",
        "                vocab.add(char)  # Add each unique character\n",
        "\n",
        "        # Step 3: Convert to sorted list for consistent ordering\n",
        "        vocab = sorted(list(vocab))\n",
        "\n",
        "        # Step 4: Add special tokens (like <UNK> for unknown words)\n",
        "        if self.unk_token not in vocab:\n",
        "            vocab.insert(0, self.unk_token)\n",
        "\n",
        "        # Step 5: Create vocabulary dictionaries\n",
        "        # vocab: maps token -> index\n",
        "        # vocab_reverse: maps index -> token\n",
        "        self.vocab = {token: idx for idx, token in enumerate(vocab)}\n",
        "        self.vocab_reverse = {idx: token for token, idx in self.vocab.items()}\n",
        "\n",
        "        # Step 6: Calculate how many merges we need to perform\n",
        "        # If we want vocab_size=1000 and we have 100 characters, we need 900 merges\n",
        "        num_merges = vocab_size - len(self.vocab)\n",
        "\n",
        "        # Step 7: Perform BPE merges iteratively\n",
        "        self.merges = []  # Store the order of merges (important for tokenization later)\n",
        "\n",
        "        for i in range(num_merges):\n",
        "            # Step 7a: Get statistics of all pairs of consecutive tokens\n",
        "            pairs = self._get_stats(word_freqs)\n",
        "\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            # Step 7b: Find the most frequent pair\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "\n",
        "            # Step 7c: Merge this pair in all words\n",
        "            word_freqs = self._merge_vocab(best_pair, word_freqs)\n",
        "\n",
        "            # Step 7d: Add the merged token to our vocabulary\n",
        "            merged_token = ''.join(best_pair)\n",
        "            if merged_token not in self.vocab:\n",
        "                self.vocab[merged_token] = len(self.vocab)\n",
        "                self.vocab_reverse[len(self.vocab) - 1] = merged_token\n",
        "\n",
        "            # Step 7e: Record this merge (we need this order for tokenization)\n",
        "            self.merges.append(best_pair)\n",
        "\n",
        "    def _apply_bpe(self, word: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Apply BPE merges to a single word.\n",
        "\n",
        "        This function takes a word and applies all the learned BPE merges in order.\n",
        "        It's like \"replaying\" the training process but for a single word.\n",
        "\n",
        "        Args:\n",
        "            word: Input word (should include </w> marker)\n",
        "\n",
        "        Returns:\n",
        "            List of BPE tokens\n",
        "        \"\"\"\n",
        "        # Step 1: Ensure word has end-of-word marker\n",
        "        word = word + self.end_of_word_token if not word.endswith(self.end_of_word_token) else word\n",
        "\n",
        "        # Step 2: Start with individual characters\n",
        "        # This is our initial state - each character is a separate token\n",
        "        tokens = list(word)\n",
        "\n",
        "        # Step 3: Apply all learned merges in the same order they were learned\n",
        "        # This is crucial! The order matters because later merges depend on earlier ones\n",
        "        for pair in self.merges:\n",
        "            new_tokens = []  # Will store tokens after applying this merge\n",
        "            i = 0  # Index to traverse current tokens\n",
        "\n",
        "            # Go through tokens and look for the pair to merge\n",
        "            while i < len(tokens):\n",
        "                # Check if current token and next token form the pair we want to merge\n",
        "                if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
        "                    # Found the pair! Merge them into a single token\n",
        "                    new_tokens.append(''.join(pair))\n",
        "                    i += 2  # Skip both tokens since we merged them\n",
        "                else:\n",
        "                    # No merge here, keep the token as is\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1  # Move to next token\n",
        "\n",
        "            # Update tokens for next iteration\n",
        "            tokens = new_tokens\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenize a text string using the trained BPE tokenizer.\n",
        "\n",
        "        Args:\n",
        "            text: Input text string\n",
        "\n",
        "        Returns:\n",
        "            List of tokenized subwords\n",
        "        \"\"\"\n",
        "        # Pre-tokenize into words\n",
        "        words = self._pre_tokenize(text)\n",
        "\n",
        "        # Apply BPE to each word\n",
        "        tokens = []\n",
        "        for word in words:\n",
        "            word_tokens = self._apply_bpe(word)\n",
        "            tokens.extend(word_tokens)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"\n",
        "        Encode text to token IDs.\n",
        "\n",
        "        Args:\n",
        "            text: Input text string\n",
        "\n",
        "        Returns:\n",
        "            List of token IDs\n",
        "        \"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        token_ids = []\n",
        "\n",
        "        for token in tokens:\n",
        "            if token in self.vocab:\n",
        "                token_ids.append(self.vocab[token])\n",
        "            else:\n",
        "                # Handle unknown tokens\n",
        "                token_ids.append(self.vocab.get(self.unk_token, 0))\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"\n",
        "        Decode token IDs back to text.\n",
        "\n",
        "        Args:\n",
        "            token_ids: List of token IDs\n",
        "\n",
        "        Returns:\n",
        "            Decoded text string\n",
        "        \"\"\"\n",
        "        tokens = [self.vocab_reverse.get(idx, self.unk_token) for idx in token_ids]\n",
        "\n",
        "        # Remove end-of-word markers and join\n",
        "        text = ''.join(tokens)\n",
        "        text = text.replace(self.end_of_word_token, ' ')\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "\n",
        "# Test the implementation\n",
        "print(\"BPE Tokenizer Implementation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Sample training texts for demonstration\n",
        "sample_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Natural language processing is fascinating.\",\n",
        "    \"Tokenization is an important preprocessing step.\"\n",
        "]\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BPETokenizer()\n",
        "\n",
        "# Train with small vocabulary size for demonstration\n",
        "print(\"\\nTraining tokenizer...\")\n",
        "tokenizer.train(sample_texts, vocab_size=50)\n",
        "\n",
        "print(f\"Vocabulary size: {len(tokenizer.vocab)}\")\n",
        "print(f\"Number of merges: {len(tokenizer.merges)}\")\n",
        "\n",
        "# Test tokenization\n",
        "test_text = \"The quick brown fox\"\n",
        "tokens = tokenizer.tokenize(test_text)\n",
        "token_ids = tokenizer.encode(test_text)\n",
        "\n",
        "print(f\"\\nTest text: '{test_text}'\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Token IDs: {token_ids}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhhWPld0zwjG"
      },
      "source": [
        "## Task 3: Tokenizer Training and Analysis (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLkP3Sofz_xV"
      },
      "source": [
        "1. Train the `BPETokenizer` on the Brown corpus with the appropriate vocabulary size selected in Task 1 (1 points)\n",
        "2. Use the Brown corpus (1000 samples) to calculate the mean and standard deviation of\n",
        "    * tokenizer's fertility (1 points)\n",
        "    * length of the tokenized sentence (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Train the BPETokenizer on the Brown corpus with the appropriate vocabulary size\n",
        "print(\"=\" * 60)\n",
        "print(\"Task 3: Tokenizer Training and Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load Brown corpus for training\n",
        "print(\"\\n1. Loading Brown corpus for training...\")\n",
        "all_sentences = brown.sents()\n",
        "training_texts = [' '.join(sent) for sent in all_sentences]\n",
        "\n",
        "print(f\"   Total training sentences: {len(training_texts):,}\")\n",
        "\n",
        "# Use vocabulary size from Task 1\n",
        "print(f\"\\n2. Training BPE tokenizer with vocabulary size: {vocab_size}\")\n",
        "tokenizer = BPETokenizer()\n",
        "tokenizer.train(training_texts, vocab_size=vocab_size)\n",
        "\n",
        "print(f\"   Actual vocabulary size: {len(tokenizer.vocab)}\")\n",
        "print(f\"   Number of BPE merges: {len(tokenizer.merges)}\")\n",
        "\n",
        "# 2. Use Brown corpus (1000 samples) to calculate statistics\n",
        "print(f\"\\n3. Getting 1000 sample sentences for analysis...\")\n",
        "sample_sentences = [' '.join(sent) for sent in brown.sents()[:1000]]\n",
        "\n",
        "# Calculate fertility (tokens per word)\n",
        "def calculate_fertility(tokenizer, sentences):\n",
        "    \"\"\"\n",
        "    Calculate fertility: ratio of tokens to words.\n",
        "\n",
        "    Fertility measures how many tokens the tokenizer produces per word.\n",
        "    - Fertility = 1.0 means each word becomes exactly 1 token\n",
        "    - Fertility > 1.0 means words are split into multiple tokens (subword tokenization)\n",
        "    - Higher fertility = more tokens = potentially more information but also more computation\n",
        "    \"\"\"\n",
        "    fertilities = []\n",
        "    for sentence in sentences:\n",
        "        # Count original words in the sentence\n",
        "        num_words = len(sentence.split())\n",
        "\n",
        "        # Tokenize the sentence using BPE (splits words into subword tokens)\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        num_tokens = len(tokens)\n",
        "\n",
        "        # Calculate fertility ratio\n",
        "        if num_words > 0:\n",
        "            fertility = num_tokens / num_words\n",
        "        else:\n",
        "            fertility = 0.0  # Handle edge case: empty sentence\n",
        "        fertilities.append(fertility)\n",
        "    return np.array(fertilities)\n",
        "\n",
        "# Calculate tokenized sentence length\n",
        "def calculate_tokenized_length(tokenizer, sentences):\n",
        "    \"\"\"Calculate length of tokenized sentences.\"\"\"\n",
        "    lengths = []\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        lengths.append(len(tokens))\n",
        "    return np.array(lengths)\n",
        "\n",
        "print(\"\\n4. Calculating fertility statistics...\")\n",
        "fertilities = calculate_fertility(tokenizer, sample_sentences)\n",
        "fertility_mean = np.mean(fertilities)\n",
        "fertility_std = np.std(fertilities)\n",
        "\n",
        "print(f\"   Mean fertility: {fertility_mean:.4f}\")\n",
        "print(f\"   Std fertility: {fertility_std:.4f}\")\n",
        "\n",
        "print(\"\\n5. Calculating tokenized sentence length statistics...\")\n",
        "lengths = calculate_tokenized_length(tokenizer, sample_sentences)\n",
        "length_mean = np.mean(lengths)\n",
        "length_std = np.std(lengths)\n",
        "\n",
        "print(f\"   Mean tokenized sentence length: {length_mean:.2f}\")\n",
        "print(f\"   Std tokenized sentence length: {length_std:.2f}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Vocabulary Size: {len(tokenizer.vocab)}\")\n",
        "print(f\"\\nFertility (tokens/word):\")\n",
        "print(f\"  Mean: {fertility_mean:.4f}\")\n",
        "print(f\"  Std:  {fertility_std:.4f}\")\n",
        "print(f\"\\nTokenized Sentence Length:\")\n",
        "print(f\"  Mean: {length_mean:.2f}\")\n",
        "print(f\"  Std:  {length_std:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grading Procedure Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During the grading of the completed assignments, a random set of students will be sampled for the **offline assignment defence**. The defence will be arranged shortly after the assignment submission deadline. The particular date and time will be announced later. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The aim of the assignment defence is to ensure the students understand well their own solutions and know how thier solution works. To check this, the students will be asked various questions about the provided solution. In addition, the students will be asked to run their solution to ensure the solution works without errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examples of questions:\n",
        "\n",
        "1. How the cumulative coverage is calculated? Why is it called cumulative?\n",
        "2. What is the rank of a word?\n",
        "3. How does the BPE tokenizer work? Note: for this question, the students will not be able to see the their own implementation.\n",
        "4. Why do you consider such vocabulary size appropriate?\n",
        "5. What is the formula for the fertility of the tokenizer?\n",
        "6. How do you perform pre-tokenization in your implementation?\n",
        "7. How do you handle stopwords in the training corpus? Why?\n",
        "8. etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a result of the assignment defence, the grade for the assignment may be adjusted."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
